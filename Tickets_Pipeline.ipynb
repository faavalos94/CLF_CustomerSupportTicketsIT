{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b96c12-5d02-45b1-ade3-cb11bd56a5ff",
   "metadata": {},
   "source": [
    "# Run pipeline job, deploy model to an endpoint, and test deployed model in Azure Machine Learning (Azure ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3cea6-f1a7-473e-afe4-8ea92bc4515c",
   "metadata": {},
   "source": [
    "A pipeline enables you to combine several tasks into a single workflow. You can create a pipeline using components, where each component represents a Python script to execute. The details of a component are defined in a YAML file, which outlines both the script and instructions for running it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e1cdd-bbb5-48e9-b222-543f4ba8d172",
   "metadata": {},
   "source": [
    "## Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06c6ae-a91e-40b0-b4f0-c4cdf7646f8a",
   "metadata": {},
   "source": [
    "The latest version of the **azure-ai-ml** package is required in order to run the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08008db-4700-4e0d-96c1-e36396cd05e1",
   "metadata": {},
   "source": [
    "Run the cell below to verify it is installed - If not, run **pip install azure-ai-ml** to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acb861-51c5-4310-a4d4-56c8be6396e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825029c7-f583-462f-ba1b-bcfd9f1c098e",
   "metadata": {},
   "source": [
    "## Connecting to your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8e24d-feac-442b-947c-ab6e4ac3a7a9",
   "metadata": {},
   "source": [
    "With the required SDK packages installed, we are ready to connect to our workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a39eb-278e-4f8e-b4c9-7cb18b42d229",
   "metadata": {},
   "source": [
    "Since we're working with a compute instance, managed by Azure ML, we can use the default credential values to connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe0bbc-8e8c-4b40-a603-44b3fa52a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bd687-5323-43d0-be8b-aff6f542e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf51f1-9131-4feb-a7a8-3b1f026300a8",
   "metadata": {},
   "source": [
    "## Configuring and registering custom environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8336f15-97f8-495a-98cb-25c7e937bdff",
   "metadata": {},
   "source": [
    "We'll set up and register a custom environment for our pipeline job by starting with a base Docker image and adding a conda specification file to add the necessary dependencies. This ensures all the required packages are installed and configured so the pipeline runs successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd927a-6e51-46a7-ad93-5e9fe22d152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "env = Environment(\n",
    "    image='mcr.microsoft.com/azureml/openmpi5.0-ubuntu24.04',\n",
    "    conda_file='conda.yml',\n",
    "    name='custom-env',\n",
    "    description='Environment with sklearn, pandas, nltk, etc.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07625894-637b-4059-b3ef-b9ded8e21005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register or update it in the workspace\n",
    "ml_client.environments.create_or_update(env)\n",
    "\n",
    "print(f\"Environment '{env.name}' registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328f4d3-6353-4131-8272-09c53ddfdbe1",
   "metadata": {},
   "source": [
    "## Create the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf0edb-fcb9-437e-be80-50613229f281",
   "metadata": {},
   "source": [
    "Our pipeline requires two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d470b-f64a-449b-a31c-f139ea1c59d0",
   "metadata": {},
   "source": [
    "1 - **Prepare the data**: Subset and clean data, and perform feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bc433-e8c8-4db0-bd60-671d7a21c0e1",
   "metadata": {},
   "source": [
    "2 - **Train the model**: Pipeline to preprocess text data, scale, and train a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da139afc-235b-41b1-914e-075d34f25495",
   "metadata": {},
   "source": [
    "Run the following cells to create the **src** folder and the two scripts described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec8b39-f166-49b5-8f84-269ae69a22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create src folder for the script files\n",
    "import os\n",
    "\n",
    "script_folder = 'src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfdc6c-dc61-4b3a-9e5d-ab98fefb0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/prep-data.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from langdetect import detect_langs, DetectorFactory\n",
    "from nltk import word_tokenize\n",
    "from pathlib import Path\n",
    "\n",
    "def main(args):\n",
    "    # Read data\n",
    "    df = get_data(args.input_data)\n",
    "\n",
    "    cleaned_data = clean_data(df)\n",
    "\n",
    "    feature_engineer_data = feature_engineer(cleaned_data)\n",
    "\n",
    "    output_df = feature_engineer_data.to_csv((Path(args.output_data) / 'customer-support-tickets.csv'), index = False)\n",
    "\n",
    "# Function that reads the data\n",
    "def get_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Count the rows and print the result\n",
    "    row_count = (len(df))\n",
    "    print('Preparing {} rows of data'.format(row_count))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function that removes missing values\n",
    "def clean_data(df):\n",
    "    # Subset data frame\n",
    "    df_set = df[['body', 'type', 'language']].copy()\n",
    "\n",
    "    # Remove missing values\n",
    "    df_set = df_set.dropna().reset_index(drop=True)\n",
    "\n",
    "    return df_set\n",
    "\n",
    "# Function to feature engineer data\n",
    "def feature_engineer(df):\n",
    "    # Ensure languages are correct and keep English 'en' tickets only\n",
    "    languages = []\n",
    "    \n",
    "    DetectorFactory.seed = 9\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        languages.append(detect_langs(df.iloc[row, 0]))\n",
    "\n",
    "    languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "    df['language'] = languages\n",
    "\n",
    "    it_ticks = df[df['language'] == 'en'].copy()\n",
    "    it_ticks.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Rename columns\n",
    "    it_ticks.rename(columns={'body': 'text', 'type': 'label'}, inplace=True)\n",
    "\n",
    "    # Create new len_words column\n",
    "    word_tokens = [word_tokenize(text) for text in it_ticks['text']]\n",
    "\n",
    "    len_tokens = []\n",
    "\n",
    "    for i in range(len(word_tokens)):\n",
    "        len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "    # Add new len_words column\n",
    "    it_ticks['len_words'] = len_tokens\n",
    "\n",
    "    # Convert ints to floats\n",
    "    it_ticks['len_words'] = it_ticks['len_words'].astype(float)\n",
    "\n",
    "    return it_ticks\n",
    "\n",
    "def parse_args():\n",
    "    # Setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Add arguments\n",
    "    parser.add_argument('--input_data', dest='input_data',\n",
    "                        type=str)\n",
    "    parser.add_argument('--output_data', dest='output_data',\n",
    "                        type=str)\n",
    "\n",
    "    # Parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Return args\n",
    "    return args\n",
    "\n",
    "# Run script\n",
    "if __name__ == '__main__':\n",
    "    # Add space in logs\n",
    "    print('\\n\\n')\n",
    "    print('*' * 60)\n",
    "\n",
    "    # Parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # Run main function\n",
    "    main(args)\n",
    "\n",
    "    # Add space in logs\n",
    "    print('*' * 60)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba03bc2-7a41-49f1-aae0-526c5661abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train-model.py\n",
    "# Import libraries\n",
    "import mlflow\n",
    "import glob\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        # Read data\n",
    "        df = get_data(args.training_data)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "        # Create pipeline to train model\n",
    "        model = create_pipeline(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # Evaluate model\n",
    "        y_pred = eval_model(model, X_test, y_test)\n",
    "\n",
    "        # Create the signature by inferring it from the datasets\n",
    "        signature = infer_signature(X_train, y_pred)\n",
    "\n",
    "        # Log the entire pipeline model\n",
    "        mlflow.sklearn.log_model(model, 'model', signature=signature)\n",
    "\n",
    "# Function that reads the data\n",
    "def get_data(data_path):\n",
    "    print('Reading data...')\n",
    "    all_files = glob.glob(data_path + '/*.csv')\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function that splits the data\n",
    "def split_data(df):\n",
    "    print('Splitting data...')\n",
    "    X, y = df[['text', 'len_words']], df['label']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to create pipeline\n",
    "def create_pipeline(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    mlflow.log_param('Regularization rate', reg_rate)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('vect', TfidfVectorizer(lowercase=False, ngram_range=(1, 2)), 'text'),\n",
    "            ('len', 'passthrough', ['len_words'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('logreg', LogisticRegression(C=1/reg_rate, max_iter=5000, random_state=9))\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "    mlflow.sklearn.save_model(model, args.model_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function that evaluates the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    # Calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print('LogisticRegression Accuracy score: {:.1%}'.format(acc_score))\n",
    "    mlflow.log_metric('Accuracy', acc_score)\n",
    "    \n",
    "    # Display confusion matrix and classification report\n",
    "    conf_matr = confusion_matrix(y_test, y_pred)\n",
    "    cla_rep = classification_report(y_test, y_pred)\n",
    "    print('\\nConfusion Matrix:\\n{}'.format(conf_matr))\n",
    "    print('\\nClassification Report:\\n{}'.format(cla_rep))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def parse_args():\n",
    "    # Setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Add arguments\n",
    "    parser.add_argument('--training_data', dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument('--reg_rate', dest='reg_rate',\n",
    "                        type=float, default=1.0)\n",
    "    parser.add_argument('--model_output', dest='model_output',\n",
    "                        type=str)\n",
    "\n",
    "    # Parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Return args\n",
    "    return args\n",
    "\n",
    "# Run script\n",
    "if __name__ == '__main__':\n",
    "    # Add space in logs\n",
    "    print('\\n\\n')\n",
    "    print('*' * 60)\n",
    "\n",
    "    # Parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # Run main function\n",
    "    main(args)\n",
    "\n",
    "    # Add space in logs\n",
    "    print('*' * 60)\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3486692-806e-4dff-afc8-7a436335fbf5",
   "metadata": {},
   "source": [
    "## Define the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b960198-f699-4caf-905f-d5f5e1cab157",
   "metadata": {},
   "source": [
    "Because our pipeline includes two steps, we need to create a separate YAML file for each component we intend to execute as part of the pipeline job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995ac33-cbc9-4437-9897-d849e45e81fe",
   "metadata": {},
   "source": [
    "A YAML file specifies details such as the **Metadata**, **Interface**, and **Command, code & environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602ab55-143b-4e02-9cf8-152eb8544bfa",
   "metadata": {},
   "source": [
    "Run the following cells to create a YAML for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bff5b-2281-432b-9720-8cde1bcefd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prep-data.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: prep_data\n",
    "display_name: Prepare training data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data: \n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:custom-env@latest\n",
    "command: >-\n",
    "  python prep-data.py \n",
    "  --input_data ${{inputs.input_data}} \n",
    "  --output_data ${{outputs.output_data}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38b764-762a-4417-9552-8fb97e45a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train-model.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train_model\n",
    "display_name: Train a logistic regression model\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  training_data: \n",
    "    type: uri_folder\n",
    "  reg_rate:\n",
    "    type: number\n",
    "    default: 1.0\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: mlflow_model\n",
    "code: ./src\n",
    "environment: azureml:custom-env@latest\n",
    "command: >-\n",
    "  python train-model.py \n",
    "  --training_data ${{inputs.training_data}}\n",
    "  --reg_rate ${{inputs.reg_rate}} \n",
    "  --model_output ${{outputs.model_output}} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fc642-7199-4b96-b7d3-7356adbb20e8",
   "metadata": {},
   "source": [
    "## Load the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4c5bd-b51a-458a-aa22-9a884adf0743",
   "metadata": {},
   "source": [
    "We will now load the components by referring to the YAML files, in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4d5bd-c604-4c4e-91ee-be60b401f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "parent_dir = ''\n",
    "\n",
    "prep_data = load_component(source=parent_dir + './prep-data.yml')\n",
    "train_logistic_regression = load_component(source=parent_dir + './train-model.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8ceb4-11d3-4005-b121-90f3bbf21fed",
   "metadata": {},
   "source": [
    "## Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759de112-4bd8-49e9-b418-e9f6d88a9624",
   "metadata": {},
   "source": [
    "Once the components are created and loaded, you can set up the pipeline by linking them together. Start by running the `prep_data` component, then use its output as the input for the `train_logistic_regression` component, which will handle the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91142635-62fb-4fde-9461-1004ae4ec57e",
   "metadata": {},
   "source": [
    "The `tickets_classification` function serves as the complete pipeline for this process. It takes one input variable: `pipeline_job_input`. A data asset was created during setup, and you'll use this registered asset as the input for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58b821-8d38-4b6b-918d-86997c1813a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline\n",
    "def tickets_classification(pipeline_job_input):\n",
    "    clean_data = prep_data(input_data=pipeline_job_input)\n",
    "    train_model = train_logistic_regression(training_data=clean_data.outputs.output_data)\n",
    "\n",
    "    return {\n",
    "        'pipeline_job_transformed_data': clean_data.outputs.output_data,\n",
    "        'pipeline_job_trained_model': train_model.outputs.model_output,\n",
    "    }\n",
    "\n",
    "pipeline_job = tickets_classification(Input(type=AssetTypes.URI_FILE, path='azureml:tickets-data:1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6a77d-9964-46de-9fc2-671f2dca834e",
   "metadata": {},
   "source": [
    "Check out the configuration of the pipeline job by printing the `pipeline_job` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515de7b-de9b-468d-9cac-ff8d0a0ad683",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964bb37-1099-4d3c-a153-6d49caacfeb6",
   "metadata": {},
   "source": [
    "You can make edits to any of the pipeline job configuration parameters by referring to the parameter and assigning the new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb93046-a6fe-4766-926f-51969991c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output mode\n",
    "pipeline_job.outputs.pipeline_job_transformed_data.mode = 'upload'\n",
    "pipeline_job.outputs.pipeline_job_trained_model.mode = 'upload'\n",
    "# Set pipeline level compute\n",
    "pipeline_job.settings.default_compute = 'aml-cluster'\n",
    "# Set pipeline level datastore\n",
    "pipeline_job.settings.default_datastore = 'workspaceblobstore'\n",
    "\n",
    "# Print the pipeline job again to review the changes\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ba6c8-571d-4d19-ac66-eb84e9b5b496",
   "metadata": {},
   "source": [
    "## Submit the pipeline job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee4298-6bd3-4cc8-bfd4-46b36e3d2e3f",
   "metadata": {},
   "source": [
    "Now that we have built the pipeline and configured the pipeline job to run as required, we can finally submit the pipeline job by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d2578-3568-4e56-966f-657ab79a45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the pipeline job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name='pipeline_tickets'\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b5ad6-21bc-4a1b-8d46-7bac8d1eccf5",
   "metadata": {},
   "source": [
    "## Register the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436deec-45ab-45de-b94e-a9850fb7b53c",
   "metadata": {},
   "source": [
    "Using MLflow in the `train-model` script, we were able to log our pipeline model. The MLmodel file stores all the model's metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5382bb-7d02-4a9a-8b05-3fd4f83d415c",
   "metadata": {},
   "source": [
    "To register our pipeline model, we will refer to the name of the `train_model` component. Registering the model as an MLflow model will make deploying it later on a simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bec34-2329-4c6e-a045-64db8288752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab train_model run_id\n",
    "train_model_run_id = None\n",
    "children = ml_client.jobs.list(parent_job_name=pipeline_job.name)\n",
    "\n",
    "for child in children:\n",
    "    if child.display_name == \"train_model\":\n",
    "        train_model_run_id = child.name  # This is the run_id for train_model\n",
    "        break\n",
    "\n",
    "print(\"Train model run ID:\", train_model_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff91d1-45ee-4740-8290-14ad18251604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model/pipeline\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "run_model = Model(\n",
    "    path=f'azureml://jobs/{train_model_run_id}/outputs/artifacts/paths/model/',\n",
    "    name='mlflow-tickets',\n",
    "    description='Model created from run.',\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    ")\n",
    "\n",
    "ml_client.models.create_or_update(run_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca1450-e1f0-4e1b-97fe-55661392e04e",
   "metadata": {},
   "source": [
    "## Define and create an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec82eb-3556-4b59-8c33-f5e899d350cb",
   "metadata": {},
   "source": [
    "Our goal is to ultimately deploy a model to a HTTPS endpoint so that an application can call to receive predictions from the model. An application can consume an endpoint by using its URI, and authenticating with a key or token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7881e-fa53-4220-a92b-4cf108c5b5d5",
   "metadata": {},
   "source": [
    "To achieve a unique endpoint name we will use the `datetime` function to generate it. Run the following cell to define the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ca71a-a134-4359-8470-f3ea22d3e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = 'endpoint-' + datetime.datetime.now().strftime('%m%d%H%M%f')\n",
    "\n",
    "# Create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description='Online endpoint for MLFlow tickets model',\n",
    "    auth_mode='key',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84246144-ff46-48af-9309-aad9dbaf1929",
   "metadata": {},
   "source": [
    "Now that the endpoint has been defined, it is time to create the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47486ad-456e-4e83-ac61-5dad3d3437fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269ed15-32b3-4232-98f1-f38f9abe5589",
   "metadata": {},
   "source": [
    "## Configure the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2485e67-e265-466e-80b3-1d875a9ad24a",
   "metadata": {},
   "source": [
    "To set up deployment, you'll need to indicate which model should be assigned to the endpoint. In the next step, you'll reference the model that was trained and saved in the local model directory. **Note: Follow steps outlined in this repo's `README.md` to save and upload model to Azure ML studio.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b237e-9127-4d96-8e41-4544a0257f85",
   "metadata": {},
   "source": [
    "Infrastructure needed for model deployment will also be specified in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8529cda-b37c-405a-be76-b98663ee95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Create blue deployment\n",
    "model = Model(\n",
    "    path='./model',\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    description='my sample mlflow model',\n",
    ")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name='blue',\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type='Standard_D2as_v4',\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ca403-4d77-4763-8b97-151a4978d406",
   "metadata": {},
   "source": [
    "## Create the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b3602-8f24-472c-a38d-203830aab083",
   "metadata": {},
   "source": [
    "Now we can deploy the model to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a84ae-7bbb-4e5b-9a51-b152f2fe5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef22ba-e3e6-47f2-bd0c-eba04cd6c91f",
   "metadata": {},
   "source": [
    "Since we only have one model deployed to the endpoint, we want this deployment to take 100% of the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3ba5a-47a3-4c44-9d92-a716081755bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {'blue': 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fd618-02be-4dd0-bd41-25097da10618",
   "metadata": {},
   "source": [
    "## Test the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18516e-eb69-4cab-9367-4752e6a3c22a",
   "metadata": {},
   "source": [
    "We will now test the deployed model by invoking the endpoint. A JSON file with a sample text and associated text word count is used as input. The trained model predicts which class an IT Customer Support ticket will be assigned to. The output can be one of four classes: **Change,** **Incident,** **Problem,** and **Request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4ac4c-324a-4b0e-b722-6d0cb2ea9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the blue deployment with some sample data\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name='blue',\n",
    "    request_file='sample-data.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab680f-f06c-4af9-83d0-74aef0b060ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a71527-7848-4b83-ba79-f9275c2051c6",
   "metadata": {},
   "source": [
    "## List endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c68839-c904-49c7-a28d-54545ba42250",
   "metadata": {},
   "source": [
    "Using the SDK, you can list all endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a9ea3-1cbf-4a42-a77d-59f6293f027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List endpoints\n",
    "endpoints = ml_client.online_endpoints.list()\n",
    "for endp in endpoints:\n",
    "    print(endp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9aca5f-74ba-4aad-91ed-e4a7e0e74e58",
   "metadata": {},
   "source": [
    "## Retrieve endpoint details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b89a9-4d75-4181-bb84-a3c1b9d7dbb3",
   "metadata": {},
   "source": [
    "If you were feeling curious and wanted more information about a specific endpoint, you can once again put the SDK to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca22581-60ae-403d-a5d8-14802a147a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint details\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# Existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd3c1b-de6c-4548-9be2-57c8d0a837d4",
   "metadata": {},
   "source": [
    "## Delete the endpoint and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab829896-a90f-45c1-90e0-593492b04384",
   "metadata": {},
   "source": [
    "Since an endpoint remains active at all times, there is no option to pause it for cost savings. To prevent incurring unnecessary charges, you should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5b657-b282-412a-b87a-e4a555916390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint and deployment - endpoints are always available and can't be paused\n",
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
